{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02_camera_calibration.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwdhiYaAQijf"
      },
      "source": [
        "# **Camera calibration**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dGUX_vCQydI"
      },
      "source": [
        "## Perspective projection (from $\\mathbb{R}^3$ to $\\mathbb{R}^2$)\r\n",
        "Let's consider:\r\n",
        "* A point in the 3d space $M = {\\begin{bmatrix} x & y & z \\end{bmatrix}}^T$, in CRF coordinates;\r\n",
        "* Its projection $m = {\\begin{bmatrix} u & v \\end{bmatrix}}^T$, onto the image plane $I$.\r\n",
        "\r\n",
        "<center> <img src=https://i.ibb.co/QcytLHX/photo-2021-01-19-16-30-07.jpg width=\"400px\" /> </center>\r\n",
        "\r\n",
        "The **non-linear** equation providing image coordinates as a function of the 3d coordinates in the CRF are:\r\n",
        "$$u = x \\frac{f}{z}$$\r\n",
        "\r\n",
        "$$v = y \\frac{f}{z}$$\r\n",
        "We would prefer linear equations, especially for noise, and so we need to linearize."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GrFoJ5zTyiJ"
      },
      "source": [
        "## Projective space (mathematical engine)\r\n",
        "The physical space is a 3d Euclidean space ($\\mathbb{R}^3$) whose points can be represented as 3d vectors in a given reference frame.\r\n",
        "In this space:\r\n",
        "* Parallel lines does not intersect (intersect at infinity);\r\n",
        "* Points at infinity cannot be represented.\r\n",
        "\r\n",
        "Starting from a generic triplet ${\\begin{pmatrix} x & y & z \\end{pmatrix}}$, it's possible to append a fourth coordinate ${\\begin{pmatrix} kx & ky & kz & k\\end{pmatrix}}$ $\\forall k \\neq 0$ and obtain the so called **homogeneous coordinates** (or projective coordinates) representation of the 3d point.\r\n",
        "\r\n",
        "The space associated with the homogeneous coordinates representation is called **projective space**, denoted as $\\mathcal{P}^3$.\r\n",
        "\r\n",
        "The extension to Euclidean spaces of any other dimension is straightforward ($\\mathbb{R}^n \\to \\mathcal{P}^n$):\r\n",
        "$${\\begin{bmatrix} x_1 \\\\ \\vdots \\\\ x_n \\end{bmatrix}} \\to k{\\begin{bmatrix} x_1 \\\\ \\vdots \\\\ x_n \\\\ x_{n + 1} = 1 \\end{bmatrix}} \\to {\\begin{bmatrix} \\frac{x_1}{x_{n+1}} \\\\ \\vdots \\\\ \\frac{x_n}{x_{n+1}} \\end{bmatrix}}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJjv8WcaX-Aj"
      },
      "source": [
        "## Points at infinity of a 3d line\r\n",
        "We already saw the Euclidean parametric equation of a 3d line:\r\n",
        "$$M = M_0 + \\lambda D = \\begin{bmatrix} x_0 \\\\ y_0 \\\\ z_0 \\end{bmatrix} + \\lambda \\begin{bmatrix} a \\\\ b \\\\ c \\end{bmatrix} = \\begin{bmatrix} x_0 + \\lambda a \\\\ y_0 + \\lambda b \\\\ z_0 + \\lambda c \\end{bmatrix}$$\r\n",
        "\r\n",
        "The 3d line parametric equation, in projective coordinates, will be:\r\n",
        "$$\\widetilde{M} = \\begin{bmatrix} M \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} x_0 + \\lambda a \\\\ y_0 + \\lambda b \\\\ z_0 + \\lambda c \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} \\frac{x_0}{\\lambda} + a \\\\ \\frac{y_0}{\\lambda} + b \\\\ \\frac{z_0}{\\lambda} + c \\\\ \\frac{1}{\\lambda} \\end{bmatrix}$$\r\n",
        "\r\n",
        "In the projective space it's possible to represent points at infinity:\r\n",
        "$$\\widetilde{M}_\\infty = \\lim_{\\lambda \\to \\infty} \\widetilde{M} = \\begin{bmatrix} a \\\\ b \\\\ c \\\\ 0 \\end{bmatrix}$$\r\n",
        "\r\n",
        "The projective coordinates of a point at infinity of a 3d line are obtained by taking any Euclidean vector parallel to the line and appending $0$ as a fourth coordinate.\r\n",
        "\r\n",
        "There exist infinitely many points at infinity in $\\mathcal{P}^3$ (as many as the directions of the 3d lines). \r\n",
        "These points cannot be represented in the Euclidean space, since for mapping them we would need to divide by $0$.\r\n",
        "\r\n",
        "*Note*: $a$, $b$ and $c$ doesn't need to be the cosine vector.\r\n",
        "\r\n",
        "With the homogeneous coordinates it is therefore possible to represent process seamlessly both ordinary points as well as points at infinity (perspective projection is more conveniently dealt using projective coordinates).\r\n",
        "\r\n",
        "Point $\\begin{pmatrix} 0 & 0 & 0 & 0 \\end{pmatrix}$ is undefined, the origin of the projective space is $\\begin{pmatrix} 0 & 0 & 0 & k \\end{pmatrix}$.\r\n",
        "\r\n",
        "All points at infinity in $\\mathcal{P}^3$ lie on a plane, called **plane at infinity**.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F25sM1Ry3QFg"
      },
      "source": [
        "## Perspective projection in projective coordinates\r\n",
        "We already know the **non-linear** equation providing image coordinates as a function of the 3d coordinates in the CRF:\r\n",
        "$$u = x \\frac{f}{z}$$\r\n",
        "\r\n",
        "$$v = y \\frac{f}{z}$$\r\n",
        "\r\n",
        "Now let's consider the 3d point $M = {\\begin{bmatrix} x & y & z \\end{bmatrix}}^T$ and its 2d projection (onto the image plane) $m = {\\begin{bmatrix} u & v \\end{bmatrix}}^T$, and represent both in homogeneous coordinates:\r\n",
        "$$\\widetilde{M} = {\\begin{bmatrix} x \\\\ y \\\\ z \\\\ 1 \\end{bmatrix}} \\in \\mathcal{P}^3$$ \r\n",
        "\r\n",
        "$$\\widetilde{m} = {\\begin{bmatrix} u \\\\ v \\\\ 1 \\end{bmatrix}} \\in \\mathcal{P}^2$$\r\n",
        "\r\n",
        "In homogeneous coordinates the perspective projection becomes a **linear transformation**:\r\n",
        "$${\\begin{bmatrix} u \\\\ v \\\\ 1 \\end{bmatrix}} = {\\begin{bmatrix} f \\frac{x}{z} \\\\ f \\frac{y}{z} \\\\ 1 \\end{bmatrix}} = {\\begin{bmatrix} fx \\\\ fy \\\\ z \\end{bmatrix}} = {\\begin{bmatrix} f & 0 & 0 & 0 \\\\ 0 & f & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\end{bmatrix}} {\\begin{bmatrix} x \\\\ y \\\\ z \\\\ 1 \\end{bmatrix}}$$\r\n",
        "\r\n",
        "In matrix notation:\r\n",
        "$$k \\widetilde{m} = \\widetilde{P} \\widetilde{M}$$\r\n",
        "\r\n",
        "or equivalently:\r\n",
        "$$\\widetilde{m} \\approx \\widetilde{P} \\widetilde{M}$$\r\n",
        "\r\n",
        "The matrix $\\widetilde{P}$ represents the geometric camera model, and is known as **perspective projection matrix** (PPM).\r\n",
        "\r\n",
        "If we assume an unitary focal lenght ($f = 1$, $8 \\text{mm}$), the PPM will be the *canonical* or *standard* PPM:\r\n",
        "$$\\widetilde{P} = {\\begin{bmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\end{bmatrix}} = {\\begin{bmatrix} I | 0 \\end{bmatrix}} $$\r\n",
        "\r\n",
        "This form is useful to understand the core operation of perspective projection: **scaling lateral coordinates $(x,y)$ according to the distance from the camera $z$**.\r\n",
        "The actual focal lenght just introduces an additional and fixed scaling factor of the projected coordinates:\r\n",
        "$$\\widetilde{m} \\approx \\widetilde{P} \\widetilde{M}$$\r\n",
        "\r\n",
        "$${\\begin{bmatrix} x \\\\ y \\\\ z \\end{bmatrix}} \\approx {\\begin{bmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\end{bmatrix}} {\\begin{bmatrix} x \\\\ y \\\\ z \\\\ 1 \\end{bmatrix}}$$\r\n",
        "\r\n",
        "Hence:\r\n",
        "$$m = \\begin{bmatrix} \\frac{x}{z} \\\\ \\frac{y}{z} \\end{bmatrix}$$\r\n",
        "\r\n",
        "*Example*\r\n",
        "\r\n",
        "Find the point at the infinity of the family of lines in $\\mathcal{P}^3$, having direction $(a,b,c)$:\r\n",
        "$$\\widetilde{m}_\\infty = {\\begin{bmatrix} fa \\\\ fb \\\\ c \\end{bmatrix}} = {\\begin{bmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\end{bmatrix}} {\\begin{bmatrix} a \\\\ b \\\\ c \\\\ 0 \\end{bmatrix}}$$\r\n",
        "\r\n",
        "On the left there is the vector of the homogeneous coordinates of the projection, in $\\mathcal{P}^2$, on the right there is the point at the infinity, in $\\mathcal{P}^3$ (the fourth coordinate $0$).\r\n",
        "Hence:\r\n",
        "$$m_\\infty = {\\begin{bmatrix} f \\frac{a}{c} \\\\ f \\frac{b}{c} \\end{bmatrix}}$$\r\n",
        "as we already saw."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0Rv99Lj0SmR"
      },
      "source": [
        "## A more comprehensive camera model\r\n",
        "For a complete model we need to take into account image digitization and the rigid motion between the CRF (camera reference frame) and the WRF (world reference frame).\r\n",
        "\r\n",
        "<center> <img src=https://i.ibb.co/CzGxWXd/photo-2021-01-20-09-04-24.jpg width=\"700px\" /> </center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NoqgI3h1Tan"
      },
      "source": [
        "### Image digitization\r\n",
        "Digitization can be taken into account including in the projection equations the scaling factors (due to quantization) associated with horizontal ($\\Delta u$) and vertical ($\\Delta v$) pixel size.\r\n",
        "\r\n",
        "Moreover we need to model the translation of the piercing point wrt the origin of the image coordinate system (top-left).\r\n",
        "\r\n",
        "<center> <img src=https://i.ibb.co/4fjWGcY/photo-2021-01-20-09-10-48.jpg width=\"500px\" /> </center>\r\n",
        "\r\n",
        "$$u = x \\frac{f}{z} \\to u = \\frac{1}{\\Delta u} \\frac{f}{z} x = k_u \\frac{f}{z} x + u_0$$\r\n",
        "\r\n",
        "$$v = y \\frac{f}{z} \\to v = \\frac{1}{\\Delta v} \\frac{f}{z} y = k_v \\frac{f}{z} y + v_0$$\r\n",
        "\r\n",
        "Tipically the scaling factors are equal ($\\Delta u \\approx \\Delta v$), $u_0$ and $v_0$ are the translation factors.\r\n",
        "\r\n",
        "The PPM becomes:\r\n",
        "$$\\widetilde{P} = {\\begin{bmatrix} f k_u & 0 & u_0 & 0 \\\\ 0 & f k_v & v_0 & 0 \\\\ 0 & 0 & 1 & 0 \\end{bmatrix}}$$\r\n",
        "\r\n",
        "We can further factorize and obtain:\r\n",
        "$$\\widetilde{P} = {\\begin{bmatrix} f k_u & 0 & u_0 \\\\ 0 & f k_v & v_0 \\\\ 0 & 0 & 1\\end{bmatrix}} \\begin{bmatrix} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\end{bmatrix} = A {\\begin{bmatrix} I | 0 \\end{bmatrix}}$$\r\n",
        "\r\n",
        "The matrix $A$, that models the characteristics of the *image sensing device*, is called **intrinsic parameter matrix**.\r\n",
        "The instrinsic parameters can be further reduced by setting:\r\n",
        "$$\\alpha_u = f k_u$$\r\n",
        "\r\n",
        "$$\\alpha_v = f k_v$$\r\n",
        "\r\n",
        "These parameters represent the focal lenght expressed in horizontal and vertical pixels.\r\n",
        "The smallest number of instrinsic parameters is thus four: $u_0$, $v_0$, $\\alpha_u$, $\\alpha_v$. \r\n",
        "A more general model include a fifth parameter called *skew*, that represents the possible non orthogonality between the axis of the image sensor.\r\n",
        "\r\n",
        "*Proof of equivalence among the representations*\r\n",
        "<center> <img src=https://i.ibb.co/N2BZFgb/photo-2021-01-20-09-35-06.jpg width=\"800px\" /> </center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNDvD0VW8JR3"
      },
      "source": [
        "### Rigid motion\r\n",
        "So far it has been assumed that 3d coordinates are measured into the CRF, but more generally coordinate are measured into the WRF, external to the camera.\r\n",
        "\r\n",
        "Let be:\r\n",
        "* $W = \\begin{bmatrix} X \\\\ Y \\\\ Z \\end{bmatrix}$ a 3d point measured in the WRF;\r\n",
        "\r\n",
        "* $M = \\begin{bmatrix} x \\\\ y \\\\ z \\end{bmatrix}$ a 3d point measured in the CRF.\r\n",
        "\r\n",
        "Then, it's possible to model the rigid motion with a rotation matrix $R$ and a translation vector $T$ (a total of six parameters).\r\n",
        "In the euclidean space the relation is non-linear:\r\n",
        "$$M = RW + T$$\r\n",
        "\r\n",
        "We can rewrite everything in homogeneous coordinates:\r\n",
        "* $\\widetilde{W} = \\begin{bmatrix} X \\\\ Y \\\\ Z \\\\ 1 \\end{bmatrix}$ a 3d point measured in the WRF;\r\n",
        "\r\n",
        "* $\\widetilde{M} = \\begin{bmatrix} x \\\\ y \\\\ z \\\\ 1 \\end{bmatrix}$ a 3d point measured in the CRF.\r\n",
        "\r\n",
        "In the projective space the relation is linear:\r\n",
        "$$\\widetilde{M} = \\begin{bmatrix} R & T \\\\ 0 & 1 \\end{bmatrix} \\widetilde{W} = G \\widetilde{W}$$\r\n",
        "\r\n",
        "So, now we can write the more general relation between an image point $M$ and world point $W$:\r\n",
        "\r\n",
        "$$k \\widetilde{m} = A \\begin{bmatrix} I | 0 \\end{bmatrix} G \\widetilde{W}$$\r\n",
        "\r\n",
        "The matrix $G$, that encodes the *position and orientation of the camera* wrt the WRF, is called **extrinsic parameter matrix**.\r\n",
        "\r\n",
        "Accordingly, the general form of the PPM will be either:\r\n",
        "\r\n",
        "$$\\widetilde{P} = A \\begin{bmatrix} I | 0 \\end{bmatrix} G$$\r\n",
        "\r\n",
        "or:\r\n",
        "\r\n",
        "$$\\widetilde{P} = A \\begin{bmatrix} R | T \\end{bmatrix}$$\r\n",
        "\r\n",
        "<center> <img src=https://i.ibb.co/tX88bG6/photo-2021-01-20-09-51-07.jpg width=\"800px\" /> </center>\r\n",
        "\r\n",
        "The general form of the PPM can be thought as: \r\n",
        "1. Encoding the position of the camera wrt the world into $G$;\r\n",
        "1. The perspective projection carried out by a pinhole camera into the canonical PPM $\\begin{bmatrix} I|0 \\end{bmatrix}$;\r\n",
        "1. The characteristics of the sensing device into $A$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epuAmoUoA28O"
      },
      "source": [
        "### Lens distortion\r\n",
        "To explain observed images we often need to model also effects of *optical distortion* induced by lenses, that are modelled as additive (non-linear) parameters, that doesn't alter the PPM.\r\n",
        "\r\n",
        "The most significant deviation from the ideal pinhole model is known as **radial distortion** (curvature), while second order effects are induced by **tangential distortion** (misalignment).\r\n",
        "\r\n",
        "The distortion is modeled through a non-linear transformation:\r\n",
        "$$\\begin{pmatrix} x' \\\\ y' \\end{pmatrix} = L(r) \\begin{pmatrix} \\widetilde{x} \\\\ \\widetilde{y} \\end{pmatrix} + \\begin{pmatrix} d \\widetilde{x} \\\\ d \\widetilde{y} \\end{pmatrix}$$\r\n",
        "\r\n",
        "where $L(r)$ is the radial distortion function, that depends on the distance $r$ from the distortion center $(\\widetilde{x}_c , \\widetilde{y}_c)$ (usually taken as piercing point):\r\n",
        "$$r = \\sqrt{(\\widetilde{x} - \\widetilde{x}_c)^2 - (\\widetilde{y} - \\widetilde{y}_c)^2}$$\r\n",
        "\r\n",
        "The radial distortion function is even, define for positive $r$ only and such as $L(0) = 1$.\r\n",
        "It's tipically approximated by his Taylor series (only even terms):\r\n",
        "$$L(r) = 1 + k_1 r^2 + k_2 r^4 + \\dots$$\r\n",
        "\r\n",
        "The tangential distortion is approxximated as:\r\n",
        "$$\\begin{pmatrix} d \\widetilde{x} \\\\ d \\widetilde{y} \\end{pmatrix} = \\begin{pmatrix} 2 p_1 \\widetilde{x} \\widetilde{y} + p_2 (r^2 + 2 \\widetilde{x}^2) \\\\ p_1 (r^2 + 2 \\widetilde{y}^2) + 2 p_2 \\widetilde{x} \\widetilde{y}\\end{pmatrix}$$\r\n",
        "\r\n",
        "The radial distortion coefficients, with the distortion center and the two tangential distortion coefficients, form a set of lens distortion parameters to build a more realistic camera model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EElDdIGfE8P4"
      },
      "source": [
        "## Calibration\r\n",
        "**Camera calibration** is the process whereby all parameters defining the camera model are estimated. \r\n",
        "Depending on the application, the whole PPM or the independent components $(A,R,T)$ are estimated.\r\n",
        "\r\n",
        "The basic process consists on setting up a linear system of equation given a set of 3d-2d correspondances, and the solve for the unknown camera parameters.\r\n",
        "To get the correspondances easily detectable features are deployed.\r\n",
        "\r\n",
        "There are two main approaches:\r\n",
        "* Single image featuring several planes;\r\n",
        "* Several images featuring a single planar plane (easier to manufacture)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMLaS886Gcuv"
      },
      "source": [
        "### Zhang's method\r\n",
        "<img src=https://i.ibb.co/JC9HQC5/Cattura.png width=\"600px\" />\r\n",
        "\r\n",
        "Given a planar chessboard pattern, are know:\r\n",
        "* The number of internal cornerns (different along the two orthogonal directions for the sake of disambiguation), easily detectable by standard algorithms such as *Harris*;\r\n",
        "* The size of the squares.\r\n",
        "\r\n",
        "In each image, the 3d WRF is taken at the top-left corner, with plane $z = 0$ and axis $x$, $y$ aligned to the two orthogonal directions (to mantain association with axis and direction, $x$ = rows and $y$ = columns).\r\n",
        "So, for a 3d point, the third coordinate is always $0$ and $x$ and $y$ are determined by the known size of the squares.\r\n",
        "\r\n",
        "To get camera parameters:\r\n",
        "1. Initial guess by linear optimization (minimization of the algebraic error);\r\n",
        "1. Refinement by non-linear optimization (minimization of the geometric error).\r\n",
        "\r\n",
        "*Note*: each image requires its own estimate of the extrinsic parameters (different positions)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQdo5O_5J82E"
      },
      "source": [
        "#### P as a homography\r\n",
        "$P$ becomes an homography when the image is planar.\r\n",
        "Due to the choice of the WRF (we consider only 3d points with $z = 0$) the PPM boils down to a simple transformation, called homography $H$:\r\n",
        "$$k \\widetilde{m} = \\widetilde{P} \\widetilde{w} = \\begin{bmatrix} p_{11} & p_{12} & p_{13} & p_{14} \\\\ p_{21} & p_{22} & p_{23} & p_{24} \\\\ p_{31} & p_{32} & p_{33} & p_{34} \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ 0 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} p_{11} & p_{12} & p_{14} \\\\ p_{21} & p_{22} & p_{24} \\\\ p_{31} & p_{32} & p_{34} \\end{bmatrix} \\begin{bmatrix} x \\\\ y \\\\ 1 \\end{bmatrix} = H \\widetilde{w}'$$\r\n",
        "\r\n",
        "An homography is a general linear transformation between planes.\r\n",
        "\r\n",
        "Given a pattern with $m$ corner, we can write $m$ systems of $3$ linear equation (as above), where both 3d and 2d coordinates are known, due to corners having been detected in the $i$-th image.\r\n",
        "The unknowns are thus the $9$ elements in $H_i$ (actually $8$, due to the arbitrary scale factor). \r\n",
        "\r\n",
        "*Example*\r\n",
        "\r\n",
        "<center> <img src=https://i.ibb.co/QPF6FbT/photo-2021-01-20-11-21-21.jpg width=\"600px\" /> </center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zBze_I3VZoo"
      },
      "source": [
        "#### Estimating $H_i$\r\n",
        "Starting from:\r\n",
        "$$k \\widetilde{m} = H \\widetilde{w}'$$\r\n",
        "\r\n",
        "we can infer that they are parallel vectors in Euclidean spaces, hence:\r\n",
        "$$\\widetilde{m} \\times H \\widetilde{w}' = 0$$\r\n",
        "\r\n",
        "To compute the vectorial product:\r\n",
        "\r\n",
        "$$\\widetilde{m} \\times H \\widetilde{w}' = \\begin{bmatrix} \\widehat{i} & \\widehat{j} & \\widehat{k} \\\\ u & v & 1 \\\\ h_1^T \\widetilde{w}' & h_2^T \\widetilde{w}' & h_3^T \\widetilde{w}' \\end{bmatrix} = \\begin{bmatrix} v h_3^T \\widetilde{w}' - h_2^T \\widetilde{w}' \\\\ h_1^T \\widetilde{w}' - u h_3^T \\widetilde{w}' \\\\ u h_2^T \\widetilde{w}' - v h_1^T \\widetilde{w}' \\end{bmatrix} = 0$$\r\n",
        "\r\n",
        "but we can rewrite that as (to put in evidence the homography terms):\r\n",
        "\r\n",
        "$$\\begin{bmatrix} 0^T & -{\\widetilde{w}'}^T & {v\\widetilde{w}'}^T \\\\ {\\widetilde{w}'}^T & 0^T & -{u\\widetilde{w}'}^T \\\\ -{v\\widetilde{w}'}^T & {u\\widetilde{w}'}^T & 0^T \\end{bmatrix} \\begin{bmatrix} h_1 \\\\ h_2 \\\\ h_3 \\end{bmatrix} = A h = 0$$\r\n",
        "\r\n",
        "We have $3$ equations in $9$ unknowns, but only $2$ of them are linearly independent (the first two of them are usually kept).\r\n",
        "\r\n",
        "By deploying all correspondances, for each image we can build a linear system of $2m$ equation in $9$ unkowns (over-constrained).\r\n",
        "\r\n",
        "This allows for achieving the initial estimation of $H_i$ by the **minimization of the algebraic error**, represented by the norm of the vector $Ah$ and enforcing the additional constraint $||h|| = 1$ (DLT algorithm).\r\n",
        "\r\n",
        "The solution to the estimation problem can be obtained by a singular value decomposition (SVD) of the matrix $A$:\r\n",
        "$$A = U \\Sigma V^T$$\r\n",
        "\r\n",
        "So we minimize the algebraic error (linear), to find an initial guess for the minimization of the geometric error (non-linear).\r\n",
        "\r\n",
        "The refinement of the previous guess is done through least-square solution of the non-linear minimization problem, with the Levenberg-Marquardt algorithm:\r\n",
        "$$h^{*} = \\text{argmin}(||Ah||^2)$$\r\n",
        "\r\n",
        "$$\\min_{H_i} \\sum_{j} {||\\widetilde{m}_j - H_i \\widetilde{w}'_j||}^2$$\r\n",
        "\r\n",
        "This additional optimization step corresponds to the minimization of the reprojection error measured for each 3d corners (for $z = 0$) of the pattern, by comparing:\r\n",
        "* The pixel coordinates predicted by the estimated homography;\r\n",
        "* The pixel coordinates of the corresponding corner extracted in the image. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RT5wysSeyyo"
      },
      "source": [
        "#### Estimation of the intrinsic parameters\r\n",
        "As $H_i$ is known up to a scale factor, we can establish the following relation between the homography and the PPM:\r\n",
        "$$H = {\\begin{bmatrix} h_1 & h_2 & h_3 \\end{bmatrix}} = {\\begin{bmatrix} p_1 & p_2 & p_3 \\end{bmatrix}} = \\lambda A {\\begin{bmatrix} r_1 & r_2 & T \\end{bmatrix}}$$\r\n",
        "\r\n",
        "$R$ is an orthogonal matrix, with orthonormal vectors.\r\n",
        "This constrains the intrinsic parameters to obey to the following relations:\r\n",
        "* Scalar product ($r_1 \\cdot r_2$) is null, since $r_1 \\perp r_2$:  \r\n",
        "\r\n",
        "$$r_1^T r_2 = 0 \\to h_1^T A^{-T} A^{-1} h_2 = 0$$\r\n",
        "\r\n",
        "* Unitary norm ($||r_1||^2$):\r\n",
        "\r\n",
        "$$r_1^T r_1 = r_2^T r_2 \\to h_1^T A^{-T} A^{-1} h_1 = h_2^T A^{-T} A^{-1} h_2$$\r\n",
        "\r\n",
        "The unknowns are the entries of $A^{-T} A^{-1}$, and since $A$ is upper triangular, $A^{-T} A^{-1}$ is symemetric, so the unknowns are just $6$.\r\n",
        "\r\n",
        "By stacking together the two equations, provided by each calibratio image, we obtain a $2n \\times 6$ linear system, that can be solved with at least $3$ images, even if however we usually put more and overconstraints it.\r\n",
        "\r\n",
        "Therefore, each image provides $2$ equations in the $6$ independent unknowns in $A^{-T} A^{-1}$, so with $n$ calibration images we get a homogeneous linear system in the form $V x = 0$, that can be solved in a least squares sense.\r\n",
        "Once $b$ is calculated, the intrinsic parameters $A$ can be obtained in closed form."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXT4PxkoDVwc"
      },
      "source": [
        "#### Estimation of the extrinsic parameters\r\n",
        "Once $A$ has been estimate, for each image it's possible to compute $R$ and $T$ given the previously computed homography $H$:\r\n",
        "\r\n",
        "$$H = {\\begin{bmatrix} h_1 & h_2 & h_3 \\end{bmatrix}} = {\\begin{bmatrix} p_1 & p_2 & p_3 \\end{bmatrix}} = \\lambda A {\\begin{bmatrix} r_1 & r_2 & T \\end{bmatrix}}$$\r\n",
        "\r\n",
        "Hence:\r\n",
        "\r\n",
        "$$h_1 = \\lambda A r_1 \\to r_1 = \\frac{1}{\\lambda} A^{-1} h_1$$\r\n",
        "\r\n",
        "As we already saw before, since $R$ is a rotation $r_1$ must be a unit vector and so we can find that $\\lambda = ||A^{-1} h_1||$.\r\n",
        "Hence we have found $r_2$ too: \r\n",
        "\r\n",
        "$$r_2 = \\frac{1}{\\lambda} A^{-1} h_2$$\r\n",
        "\r\n",
        "Moreover $r_3$ can be derived exploiting orthonormality: $r_3 = r_1 \\times r_2$.\r\n",
        "\r\n",
        "Now we can finally get $T$:\r\n",
        "$$T = \\frac{1}{\\lambda} A^{-1} h_3$$\r\n",
        "\r\n",
        "*Note*: actually $R^*$ is not a perfectly rotation matrix."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JiiMwRdSFi3d"
      },
      "source": [
        "#### Lens distortion coefficients\r\n",
        "Given the homographies, we have both real distorted coordinates of the corner features found in the images, and the ideal undistorted as well (predicted by the homographies).\r\n",
        "\r\n",
        "Given the distortion model:\r\n",
        "\r\n",
        "$$\\begin{pmatrix} x' \\\\ y' \\end{pmatrix} = L(r) \\begin{pmatrix} \\widetilde{x} \\\\ \\widetilde{y} \\end{pmatrix} = (1 + k_1 r^2 + k_2 r^4) \\begin{pmatrix} \\widetilde{x} \\\\ \\widetilde{y} \\end{pmatrix}$$\r\n",
        "\r\n",
        "In terms of perspective projection:\r\n",
        "\r\n",
        "$${\\begin{bmatrix} u' \\\\ v' \\\\ 1 \\end{bmatrix}} = A {\\begin{bmatrix} x' \\\\ y' \\\\ 1 \\end{bmatrix}} = {\\begin{pmatrix} \\alpha_u & 0 & u_0 \\\\ 0 & \\alpha_v & v_0 \\\\ 0 & 0 & 1 \\end{pmatrix}} {\\begin{bmatrix} x' \\\\ y' \\\\ 1 \\end{bmatrix}}$$\r\n",
        "\r\n",
        "So we have:\r\n",
        "\r\n",
        "$$x' = \\frac{u' - u_0}{\\alpha_u}$$\r\n",
        "\r\n",
        "$$y' = \\frac{v' - v_0}{\\alpha_v}$$\r\n",
        "\r\n",
        "Moreover:\r\n",
        "\r\n",
        "$${\\begin{bmatrix} \\widetilde{u} \\\\ \\widetilde{v} \\\\ 1 \\end{bmatrix}} = A {\\begin{bmatrix} \\widetilde{x} \\\\ \\widetilde{y} \\\\ 1 \\end{bmatrix}}$$\r\n",
        "\r\n",
        "Hence:\r\n",
        "\r\n",
        "$$\\widetilde{x} = \\frac{\\widetilde{u} - u_0}{\\alpha_u}$$\r\n",
        "\r\n",
        "$$\\widetilde{y} = \\frac{\\widetilde{v} - v_0}{\\alpha_v}$$\r\n",
        "\r\n",
        "From the distortion model we know that:\r\n",
        "\r\n",
        "$$\\begin{pmatrix} x' \\\\ y' \\end{pmatrix} = (1 + k_1 r^2 + k_2 r^4) \\begin{pmatrix} \\widetilde{x} \\\\ \\widetilde{y} \\end{pmatrix}$$\r\n",
        "\r\n",
        "Substituting the previously found relation:\r\n",
        "\r\n",
        "$$\\frac{u' - u_0}{\\alpha_u} = (1 + k_1 r^2 + k_2 r^4) \\frac{\\widetilde{u} - u_0}{\\alpha_u}$$\r\n",
        "\r\n",
        "$$\\frac{v' - v_0}{\\alpha_v} = (1 + k_1 r^2 + k_2 r^4) \\frac{\\widetilde{v} - v_0}{\\alpha_v}$$\r\n",
        "\r\n",
        "Rearranging:\r\n",
        "\r\n",
        "$$u' = \\widetilde{u} + (k_1 r^2 + k_2 r^4)(\\widetilde{u} - u_0)$$\r\n",
        "\r\n",
        "$$v' = \\widetilde{v} + (k_1 r^2 + k_2 r^4)(\\widetilde{v} - v_0)$$\r\n",
        "\r\n",
        "Is therefore possible to set up a linear system where the unknowns are the distortion coefficients:\r\n",
        "\r\n",
        "$$\\begin{bmatrix} (\\widetilde{u} - u_0) r^2 & (\\widetilde{u} - u_0) r^4 \\\\ (\\widetilde{v} - v_0) r^2 & (\\widetilde{v} - v_0) r^4 \\end{bmatrix} \\begin{bmatrix} k_1 \\\\ k_2\\end{bmatrix} = \\begin{bmatrix} u' - \\widetilde{u} \\\\ v' - \\widetilde{v} \\end{bmatrix}$$\r\n",
        "\r\n",
        "where:\r\n",
        "\r\n",
        "$$r = {\\frac{\\widetilde{u} - u_0}{\\alpha_u}}^2 + {\\frac{\\widetilde{v} - v_0}{\\alpha_v}}^2$$\r\n",
        "\r\n",
        "With $m$ corner features in $n$ images, we get a linear system with $2mn$ equations in $2$ unknowns, which admits a least-squares solution (**normal equation**):\r\n",
        "\r\n",
        "$$D k = d \\to k = D^+ d = (D^T D)^{-1} D^T d$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSs5ytrrLuGG"
      },
      "source": [
        "#### Refinement by non-linear optimization\r\n",
        "After we minimize the algebraic error to find an initial guess (with no physical meaning), a more accurate solution can be found minimizing the geometric error (reprojection error), with the so called **maximum likelihood estimate** (MLE).\r\n",
        "\r\n",
        "Under the hypotesis of IID (independent identically distribuited) noise, the MLE for our models is obtained by minimization of the error:\r\n",
        "\r\n",
        "$$\\sum_{i = 1}^{n} \\sum_{j = 1}^{m} {||m_{ij} - \\hat{m}(A,k,R_i,T_i,w_j)||}^2$$\r\n",
        "\r\n",
        "wrt all the unknown camera parameters.\r\n",
        "\r\n",
        "The solution is provided by the Levenberg-Marquardt algorithm (iterative method)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsiKJ3waNJaS"
      },
      "source": [
        "## From pixels to 3d coordinates and back to pixels\r\n",
        "If the camera is calibrated (matrix $A$ is known, the depth $z$ is known), for a depth camera (i.e. Kinect), it's possible to estimate the 3d coordinates of the pixels in the CRF, in order to obtain a so called **point cloud** (a representation that used to be quite rare, but that's now widely used in 3d computer vision).\r\n",
        "\r\n",
        "Let be:\r\n",
        "* $P$ the 3d coordinate in the CRF;\r\n",
        "* $p$ the 2d pixel coordinate.\r\n",
        "\r\n",
        "Then, it holds:\r\n",
        "\r\n",
        "$$ \\begin{bmatrix} \\alpha_u x + u_0 z \\\\ \\alpha_v y + v_0 z \\\\ z \\end{bmatrix} = \\begin{bmatrix} \\alpha_u x & 0 & u_0 \\\\ 0 & \\alpha_v & v_0 \\\\ 0 & 0 & 1 \\end{bmatrix} \\begin{bmatrix} z \\\\ y \\\\ z \\end{bmatrix}$$\r\n",
        "\r\n",
        "$$p^* = AP$$\r\n",
        "\r\n",
        "Isolating $P$:\r\n",
        "\r\n",
        "$$A^{-1} p^* = P$$\r\n",
        "\r\n",
        "Multiplying and dividing by $z$:\r\n",
        "\r\n",
        "$$P = z A^{-1} \\frac{p^*}{z}$$\r\n",
        "\r\n",
        "But we know that:\r\n",
        "\r\n",
        "$$\\frac{p^*}{z} = \\begin{bmatrix} \\alpha_u \\frac{x}{z} + u_0 \\\\ \\alpha_v \\frac{y}{z} + v_0 \\\\ 1 \\end{bmatrix} = \\begin{bmatrix} u \\\\ v \\\\ 1 \\end{bmatrix} = p$$\r\n",
        "\r\n",
        "Thus:\r\n",
        "$$P = z A^{-1} p$$\r\n",
        "\r\n",
        "Moreover, we may wish to find where a certai pixels project into another image, taken by the same camera (i.e. moving camera to scan a static scene).\r\n",
        "We need to know the rigid motion too ($R$ and $T$) between the two views:\r\n",
        "\r\n",
        "<center> <img src=https://i.ibb.co/5G6Gxgq/photo-2021-01-20-16-39-43.jpg\r\n",
        " width=\"600px\" /> </center>\r\n",
        "\r\n",
        "Or maybe the other image is taken by a different camera: we would need to know the intrinsic parameters of the other camera too:\r\n",
        "\r\n",
        "\r\n",
        "<center> <img src=https://i.ibb.co/LxgbQHD/photo-2021-01-20-16-43-28.jpg\r\n",
        " width=\"600px\" /> </center>\r\n",
        "\r\n",
        "*Note*\r\n",
        "\r\n",
        "Humans developed a monocular depth vision: depht from mono, acquirable with a normal camera (eye) and machine learning (memory and brain). \r\n",
        "However this is quite impratical with supervised learning (how can we provide samples?).\r\n",
        "\r\n",
        "Then we need to use unsupervise learning, or better, self-supervised learning, using the image reconstruction error $|I(p_1) - I(p_2)|$ to drive the learning of CNN (convolution neural networks).  \r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hovZrBolfAkr"
      },
      "source": [
        "## Image warping\r\n",
        "A **warping** is a transformation $f$ between pixels coordinates:\r\n",
        "$$u' = f_u (u , v)$$\r\n",
        "$$v' = f_v (u , v)$$\r\n",
        "Such that $I'\\bigl(f_u (u , v), f_v (u , v)\\bigr) = I(u , v)$.\r\n",
        "\r\n",
        "<center> <img src=https://i.ibb.co/2symDWN/photo-2021-01-20-20-20-42.jpg width=\"500px\" /> </center>\r\n",
        "\r\n",
        "*Example*\r\n",
        "* **Rotation**\r\n",
        "$$\\begin{bmatrix} u' \\\\ v' \\end{bmatrix} = \\begin{pmatrix} \\cos \\theta & - \\sin \\theta \\\\ \\sin \\theta & \\cos \\theta \\end{pmatrix} \\begin{bmatrix} u \\\\ v \\end{bmatrix}$$\r\n",
        "\r\n",
        "* **Removal of perspective deformation**\r\n",
        "$$s \\begin{bmatrix} x' \\\\ y' \\\\ w' \\end{bmatrix} = \\begin{pmatrix} h_{11} & h_{12} & h_{13} \\\\ h_{21} & h_{22} & h_{23} \\\\ h_{31} & h_{32} & h_{33} \\end{pmatrix} \\begin{bmatrix} x \\\\ y \\\\ z \\end{bmatrix}$$\r\n",
        "\r\n",
        "* **Correct lens distortion**\r\n",
        "\r\n",
        "<center> <img src=https://i.ibb.co/3f4g3Ms/photo-2021-01-20-20-35-54.jpg width=\"500px\" /> </center>\r\n",
        "\r\n",
        "Once that lens distortion parameters have been computed by camera calibration, the image can be corrected by a **backward mapping**:\r\n",
        "$$\\forall(\\tilde{u},\\tilde{v}) \\colon \\quad I'(\\tilde{u},\\tilde{v}) = I \\bigl(g_u (\\tilde{u},\\tilde{v}), g_v (\\tilde{u},\\tilde{v}) \\bigr)$$\r\n",
        "\r\n",
        "Using the Zhang's calibration method we we'll have:\r\n",
        "$$u' = \\tilde{u} + (k_1 r^2 + k_2 r^4)(\\tilde{u} - u_0)$$\r\n",
        "\r\n",
        "$$v' = \\tilde{v} + (k_1 r^2 + k_2 r^4)(\\tilde{v} - v_0)$$\r\n",
        "\r\n",
        "* **Stereo rectification**.\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFNXcnsoU5vd"
      },
      "source": [
        "### How to correctly perform a warping? Mapping strategies\r\n",
        "* **Forward mapping**: in this manner the coordinates are mapped into new coordinates, i.e. $(u_k',v_k') = (\\lfloor u' \\rfloor, \\lfloor v' \\rfloor)$, but in this way we'll have holes and folds in $I'(u',v')$.\r\n",
        "When we transfer the forward intensity we have no guarantee that each pixel is mapped (holes), or even worse we could have multiple assignments (folds).\r\n",
        "\r\n",
        "<center> <img src=https://i.ibb.co/j86Jwk7/photo-2021-01-20-20-45-36.jpg width=\"500px\" /> </center>\r\n",
        "\r\n",
        "* **Backward mapping**: in this manner we don't have holes nor folds in $I'(u',v')$, even though we still have the discretization problem $(u_k , v_k) = (\\lfloor u \\rfloor, \\lfloor v \\rfloor)$.\r\n",
        "\r\n",
        "<center> <img src=https://i.ibb.co/9cnDNGm/photo-2021-01-20-20-56-54.jpg width=\"500px\" /> </center>\r\n",
        "\r\n",
        "$$u = g_u (u' , v')$$\r\n",
        "$$v = g_v (u' , v')$$\r\n",
        "\r\n",
        "$$\\forall(u' , v') \\colon \\quad I'(u' , v') = I \\bigl(g_u (u' , v'), g_v (u' , v') \\bigr)$$\r\n",
        "\r\n",
        "* **Mapping from the closest point** (Nearest neighbour mapping);\r\n",
        "\r\n",
        "* **Interpolation between the 4 closest point** (bilinear, bicubic,...)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZJmhrfwZLlU"
      },
      "source": [
        "#### Bilinear interpolation\r\n",
        "It's the simpler and most popular mapping strategy since it's fast and accurate enough.\r\n",
        "\r\n",
        "Let's consider the fractional part of the real valued coordinates (not pixels!):\r\n",
        "$$\\Delta u = u - u_k$$\r\n",
        "$$\\Delta v = v - v_k$$\r\n",
        "\r\n",
        "And the 4 closest points:\r\n",
        "$$I_1 = I(u_k, v_k)$$\r\n",
        "$$I_2 = I(u_k + 1, v_k)$$\r\n",
        "$$I_3 = I(u_k, v_k + 1)$$\r\n",
        "$$I_4 = I(u_k + 1, v_k + 1)$$\r\n",
        "\r\n",
        "\r\n",
        "<center> <img src=https://i.ibb.co/ZHMNycR/photo-2021-01-20-21-07-23.jpg width=\"800px\" /> </center>\r\n",
        "\r\n",
        "Re-arranging we will have:\r\n",
        "$$I'(u',v') = (1 - \\Delta u) (1 - \\Delta v) I_1 + \\Delta u (1 - \\Delta v) I_2 + (1 - \\Delta u) \\Delta v I_3 + \\Delta u \\Delta v I_4$$\r\n",
        "\r\n",
        "that is the weighted sum of the input intensities."
      ]
    }
  ]
}