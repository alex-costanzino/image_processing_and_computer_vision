{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"01_image_formation_and_acquisition.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyP90hKIdJnZzm8Wv0TQoCnm"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"YuNTYy2XZgq3"},"source":["# **Image formation and acquisition**\r\n","An *imaging device* gathers the light reflected by 3D objects to create a 2D representation of the scene (an image).\r\n","In *computer vision* the process is inverted: the goal is to infer knowledge on the objects from one or more digital images.\r\n","\r\n","To understand this process is required the knowledge about:\r\n","* The geometric relationship between **scene points** (3D) and **image points** (2D);\r\n","* The radiometric relationship between the brightness of image points and the light emitted by scene points;\r\n","* The image digitization process (quantization)."]},{"cell_type":"markdown","metadata":{"id":"rlmuH5JIa42R"},"source":["## The pinhole camera\r\n","A pinhole camera is the simplest imaging device: the light (ideally only one ray) goes through a very small hole and hits the image plane (sensor or photosensitive paper).\r\n","\r\n","Since the image is obtained by *drawing* straight rays from scene points, through the hole, to the image plane, the images appear reversed.\r\n","\r\n","Useful images hardly can be captured by such device, nevertheless it's a simple geometrical model to approximate the image formation process.\r\n"]},{"cell_type":"markdown","metadata":{"id":"vv11F8h1c5yH"},"source":["## Perspective projection\r\n","The geometric model of image formation in a pinhole camera is known as **perspective projection**.\r\n","\r\n","<img src=\"https://i.ibb.co/R0bhRwj/photo-2021-01-18-13-26-00.jpg\" width=\"400px\" />\r\n","\r\n","Where:\r\n","* $M$ is a scene point and $m$ is the corresponding image point;\r\n","* $I$ is the image plane;\r\n","* $C$ is the optical center and the line through $C$, orthogonal to $I$ is the optical axis;\r\n","* $c$ is the piercing point (or image center) and it's the interesction between the opticaa axis and the image plane.\r\n","* $F$ is the focal plane: a plane parallel to the image plane and passing through the piercing point;\r\n","* $f$ is the focal lenght (distance between piercing point and optical center).\r\n","\r\n","The goal is to find a mapping between world coordinates (3D) and image coordinates (2D): $(x,y,x) \\to (u,v)$.\r\n","\r\n","*Note*: we measure the coordinate in their own reference system: camera reference system (CRS) for 3d point and image reference system (IRS) for 2d points. The piercing point $c$ is the origin for the IRS and the optical center $C$ is the origin for the CRS\r\n","\r\n","<img src=\"https://i.ibb.co/KhW6LjS/photo-2021-01-18-13-45-02.jpg\" width=\"1000px\" />\r\n","\r\n","Due to triangles similarity, it holds: $$\\frac{u}{x} = \\frac{v}{y} = -\\frac{f}{z}$$\r\n","Frome the above relationship we can derive:\r\n","$$u = -x \\frac{f}{z}$$\r\n","$$v = -y \\frac{f}{z}$$\r\n","\r\n","We can get rid of sign inversion if we think that the image plane is in front rather than behind the optical center:\r\n","$$u = x \\frac{f}{z}$$\r\n","$$v = y \\frac{f}{z}$$\r\n","\r\n","What these equation tell us?\r\n","1. They map 3d coordinates into 2d coordinates;\r\n","1. Image coordinates are just a **scaled version** of the scene ones, scaled by a factor $\\frac{f}{z}$.\r\n","1. We can find a point in the image given it's position in the real world.\r\n","\r\n","About the last point, since image formation process deals with mapping a 3d space onto a 2d space, it leads to a inevitably loss of information.\r\n","Indeed, the mapping is not a bijection: a given scene point is mapped into a unique image point, but a given image point is mapped onto a 3d line.\r\n","\r\n","Recovering the 3d structure is an ill-posed problem (no unique solution): once we take an image point we can only state that its corresponding scene point lays on a line, but we can't distinguish the right point.\r\n","\r\n","A basic approach to address this problem is use more images (stereo vision, is that why we have two eyes).\r\n"]},{"cell_type":"markdown","metadata":{"id":"6AAYCc_LpoIg"},"source":["## Stereo images for 3d inference\r\n","Given correspondances, 3d information can be recovered easily by triangulation: two constraints to localize the points in the 3d space.\r\n","\r\n","<img src=\"https://i.ibb.co/6WgxQBr/photo-2021-01-18-14-22-09.jpg\" width=\"800px\" />\r\n","\r\n","The two planes $\\pi_L$ and $\\pi_R$ don't have to be in the same plane, but if planes are parallel we simplify the problem (like human eyes)."]},{"cell_type":"markdown","metadata":{"id":"R8z_Gal8oxqx"},"source":["### Standard stereo geometry\r\n","It's the one typically used in cameras.\r\n","* Parallel $(x,y,z)$ axes like human eyes;\r\n","* Same focal lenght (same camera);\r\n","* Coplanar image planes.\r\n","\r\n","Let be:\r\n","* $P_L = \\begin{bmatrix} x_L \\\\ y_L \\\\ z_L \\end{bmatrix}$ a point in the left CRS;\r\n","* $P_R = \\begin{bmatrix} x_R \\\\ y_R \\\\ z_R \\end{bmatrix}$ a point in the right CRS.\r\n","\r\n","The transformation between the two reference frames is just a translation $b$, usually horizontal:\r\n","$$P_L - P_R = \\begin{bmatrix} x_L - x_R \\\\ y_L - y_R \\\\ z_L - z_R \\end{bmatrix} = \\begin{bmatrix} b \\\\ 0 \\\\ 0 \\end{bmatrix}$$\r\n","Now considering the **perspective projection equation** (we have the same vertical height):\r\n","$$v_L = v_R = y \\frac{f}{z}$$\r\n","$$u_L = x_L \\frac{f}{z}$$\r\n","$$u_R = x_R \\frac{f}{z}$$\r\n","It's possible to calculate the disparity $d = u_L - u_R$ (the horizontal distance between the two points):\r\n","$$d = u_L - u_R = x_L \\frac{f}{z} - x_R \\frac{f}{z} = (x_L - x_R) \\frac{f}{z} = b \\frac{f}{z}$$\r\n","Hence:\r\n","$$d = b \\frac{f}{z}$$\r\n","Extracting $z$, the **fundamental relation of stereo vision** is obtained:\r\n","$$z = b \\frac{f}{d}$$\r\n","\r\n","So we measure the disparity $d$ and then compute the $z$. \r\n","Note that there are no info about the correspondances: to find for example $P_R$ (remembering that $v_L=v_R$) just search at the same height (the candidates are the pixels on the red line).\r\n","\r\n","*Note*: windows of pixels are used since it's not possible to find the exact correspondance with just one pixel.\r\n","\r\n","<img src=\"https://i.ibb.co/pb8p72v/photo-2021-01-18-17-07-32.jpg\" width=\"800px\" />\r\n","\r\n","**Projector**: device that projects some random black dots to infer some knowledge about the geometry in non-uniform areas."]},{"cell_type":"markdown","metadata":{"id":"d1psVEg4o0s1"},"source":["### Epipolar geometry\r\n","It's an alternative stereo vision approach, based on the fact that all the epipolar lines in an image meet at a point called epipole (the projection of the optical center of the other image). Usually is used as a second mapping to check the correctness.\r\n","\r\n","<img src=\"https://i.ibb.co/RjVVxbS/photo-2021-01-18-17-12-51.jpg\" width=\"400px\" />\r\n","\r\n","However, searching through oblique epipolar lines is awkward, so a rectification can be used.\r\n","It's always possible to warp the images as if they were acquired through a standard geometry (horizontal and collinear conjugate epipolar lines) by computing and applying to both a transformation (an homography) to rectificate."]},{"cell_type":"markdown","metadata":{"id":"9Dr9qaxRo7TW"},"source":["### The stereo correspondance problem\r\n","Given a point in one image (left camera) find that in the other image (right camera) which is the projection of the same 3d point (corresponding point).\r\n","The basic idea is that corresponding points look similar in the two images."]},{"cell_type":"markdown","metadata":{"id":"5dAk0WikUPAX"},"source":["## Properties of perspective projection\r\n","* The farther objects are from the camera, the smaller they appear in the image. If planes are perfectly aligned it holds $l = L  \\frac{f}{z}$, but in general, even with arbitrarily position and orientation, **lenght always shrinks alongside distance**;\r\n","* Perspective projection maps 3d lines into images lines (in theory);\r\n","* Ratios of lenghts are not preserved unless the sceneis planar and parallel to the image plane;\r\n","* Parallelism between 3d lines is not preserved excepts for lines parallel to the image plane (**perspective distortion**).\r\n","\r\n","<center><img src=https://i.ibb.co/XVmgC6S/photo-2021-01-18-17-37-06.jpg width=\"400px\" /></center>\r\n"]},{"cell_type":"markdown","metadata":{"id":"hyOzqemAkKnc"},"source":["## Vanishing points\r\n","The images of parallel 3d lines (in real world) meet at a point, referred as vanishing point.\r\n","\r\n","The vanishing point of a 3d line is the **image of the point at infinity of the line** and it can be determined by the intersection between: \r\n","* The image plane;\r\n","* The line parallel to the given one, passing through the optical center.\r\n","\r\n","So, all parallel 3d lines will share the same vanishing point (they meet at their vanishing point in the image, if such point is at infinity).\r\n","\r\n","Let's consider the parametric equation of the line in the CRF:\r\n","$$M = M_0 + \\lambda D = \\begin{bmatrix} x_0 \\\\ y_0 \\\\ z_0 \\end{bmatrix} + \\lambda \\begin{bmatrix} a \\\\ b \\\\ c \\end{bmatrix}$$\r\n","where $M_0$ is a point on the line and $D$ is the direction cosines vector.\r\n","\r\n","First project a generic point $m$ of the line. Given:\r\n","$$m = \\begin{bmatrix} u \\\\ v \\end{bmatrix}$$\r\n","the projection will be:\r\n","$$u = f \\frac{x}{z} = f \\frac{x_0 + \\lambda a}{z_0 + \\lambda c}$$\r\n","$$v = f \\frac{y}{z} = f \\frac{y_0 + \\lambda b}{z_0 + \\lambda c}$$\r\n","To get the vanishing point we consider the **infinitely distant point** along the line. Given:\r\n","$$m_\\infty = \\begin{bmatrix} u_\\infty \\\\ v_\\infty \\end{bmatrix}$$\r\n","the projection will be:\r\n","$$u_\\infty = \\lim_{\\lambda \\to \\infty} u = f \\frac{a}{c}$$\r\n","$$v_\\infty = \\lim_{\\lambda \\to \\infty} v = f \\frac{b}{c}$$\r\n","\r\n","The vanishing point depends on the orientation of the line only ($a$, $b$ and $c$), not on its position.\r\n","Whenever the line is parallel to the image plane ($c = 0$) it goes to infinity.\r\n","In such case the image of the line has the same orientation as the 3d line.\r\n","\r\n","Knowledge of a vanishing point of a sheaf of parallel lines (and the focal lenght) allows for determining the unknown orientation of the lines.\r\n","\r\n","We know that:\r\n","* $u_\\infty = f \\frac{a}{c}$;\r\n","* $v_\\infty = f \\frac{b}{c}$;\r\n","* $a^2 + b^2 + c^2 = 1$ (unit vector).\r\n","\r\n","So:\r\n","$$u_\\infty^2 + v_\\infty^2 = f^2 \\frac{a^2}{c^2} + f^2 \\frac{b^2}{c^2} = \\frac{f^2}{c^2}(a^2 + b^2)$$\r\n","Multiplying for $c^2$, and since that $a^2 + b^2 = 1 - c^2$:\r\n","$$c^2(u_\\infty^2 + v_\\infty^2) = f^2 (1 - c^2)$$\r\n","Expanding and rearranging:\r\n","$$c^2(u_\\infty^2 + v_\\infty^2) = f^2 - f^2 c^2$$\r\n","$$c^2(u_\\infty^2 + v_\\infty^2) + f^2 c^2 = f^2$$\r\n","$$c^2(u_\\infty^2 + v_\\infty^2 + f^2) = f^2$$\r\n","Squaring, we finally obtain:\r\n","$$c = \\frac{f}{\\sqrt{u_\\infty^2 + v_\\infty^2 + f^2}}$$\r\n","\r\n","Reversing the first two relationship, we obtain:\r\n","$$a = u_\\infty \\frac{c}{f}$$\r\n","$$b = v_\\infty \\frac{c}{f}$$\r\n","Making a substituion:\r\n","$$a = \\frac{u_\\infty}{\\sqrt{u_\\infty^2 + v_\\infty^2 + f^2}}$$\r\n","$$b = \\frac{v_\\infty}{\\sqrt{u_\\infty^2 + v_\\infty^2 + f^2}}$$\r\n","\r\n","So we can write in a more compact way:\r\n","$$\\begin{bmatrix} a \\\\ b \\\\ c \\end{bmatrix} = \\frac{1}{\\sqrt{u_\\infty^2 + v_\\infty^2 + f^2}} \\begin{bmatrix} u_\\infty \\\\ v_\\infty \\\\ f \\end{bmatrix}$$\r\n","\r\n","*Example application*: a mobile robot can be driven through indoor hallways by tracking the dominant vanishing point and steering the robot to keep it at the center of the image"]},{"cell_type":"markdown","metadata":{"id":"Y-qMccPlpBUS"},"source":["### Camera orientation from vanishing point\r\n","Knowledge of vanishing points of two orthogonal directions allows determining camera orientation wrt a scene plane.\r\n","\r\n","From the horizontal lines we get the unit vector $i_c$, while from the vertical ones we get the unit vector $j_c$.\r\n","The vector product between the two provides $k_c = i_c \\times j_c$.\r\n","\r\n","These three unit vectors define the orientation of the camera wrt the considered scene plane: \r\n","$$R_{\\text{pc}} = \\begin{bmatrix} i_c & j_c & k_c \\end{bmatrix}$$\r\n","$$R_{\\text{cp}} = {R_{\\text{pc}}}^T$$"]},{"cell_type":"markdown","metadata":{"id":"6kXpCj2VkWul"},"source":["## Weak perspective\r\n","Perspective effects may be not so evident, this occours whenever the framed subject is thin compared to the distance from the camera.\r\n","In such cases perspective projection can be approximated by a scaled orthographic projection (weak perspective, basically we project with straight lines):\r\n","$$ \\begin{cases} u = sx \\\\ v = sy \\end{cases}$$\r\n","\r\n","When apply it? When we are looking at a thin scene with a large distance (the range of depth is small compared to the depth).\r\n","\r\n","<img src=https://i.ibb.co/MNzzdBV/photo-2021-01-18-18-43-28.jpg width=\"400px\" />\r\n","\r\n","Since $\\Delta z << z_0$ it will be:\r\n","$$\\frac{f}{z_0 + \\Delta z} = \\frac{f}{z_0 - \\Delta z} = \\frac{f}{z_0} = s$$\r\n","\r\n","And then:\r\n","$$u \\approx \\frac{f}{z_0} x = sx $$\r\n","$$v \\approx \\frac{f}{z_0} y = sy $$\r\n","\r\n","It means that the perspective scaling factor $\\frac{f}{z}$ is approximately constant $\\frac{f}{z_0}$ for all points of the framed subject."]},{"cell_type":"markdown","metadata":{"id":"CbEwhHgkS3bU"},"source":["## Lenses\r\n","The lense gather light and focuses it in a single point.\r\n","\r\n","A scene point is on focus when all its light rays gathered by the camera hit the image plane at the same point.\r\n","\r\n","In a pinhole device this happens to all scene points, due to the small size of the hole.\r\n","The camera features an infinite **depth of field** (DOF). \r\n","The drawback is that such small aperture allows gathering a very limited amount of light, so very long exposure time is required to avoid blurs (so only static scenes can be acquired to avoid the so called *motion blur*).\r\n","\r\n","For this reason, cameras rely on lenses to gather more light from a scene point and focus it in a single image point, enabling a smaller exposure time.\r\n","However the DOF is no longer infinite, since only points across a limited range of distances can be simultaneously on focus in a given image."]},{"cell_type":"markdown","metadata":{"id":"_oqzsA74VOh9"},"source":["### Thin lens equation\r\n","In general cameras feature complex optical system, but it's possible to use an approximate model called **thin lens model**.\r\n","\r\n","<img src=https://i.ibb.co/qkV8tpr/photo-2021-01-18-22-11-32.jpg width=\"400px\" />\r\n","\r\n","Let be:\r\n","* $P$ the scene point (3d) and $p$ the corresponding **focused** image point (2d);\r\n","* $u$ the distance from $P$ to the lens and $v$ the distance $p$ to the lens;\r\n","* $f$ the focal lenght (now a lens parameter);\r\n","* $C$ the center of the lens;\r\n","* $F$ the focal point or focus of the lens.\r\n","\r\n","Then it hold: $$\\frac{1}{u} + \\frac{1}{v} = \\frac{1}{f}$$\r\n","\r\n","To determine the position of a focused image point we can leverage on the properties of thin lenses:\r\n","1. Rays parallel to the optical axis are deflected and pass through $F$;\r\n","1. Rays through $C$ are undeflected.\r\n","\r\n","That means that we can focus only at a given distance.\r\n","\r\n","*Note*: if the image is on focus, the image formation process obeys to the perspective projection model, with the center of the lens being the optical center and the distance $v$ acting as the effective focal lengh of the projection, that is a different concept wrt the focal lenght of the lens."]},{"cell_type":"markdown","metadata":{"id":"7Iv7AaXwYFzA"},"source":["### Circle of confusions\r\n","Due the thin lens equation we can:\r\n","1. Fix the image distance $v$ of the image plane: $$\\frac{1}{u} + \\frac{1}{v} = \\frac{1}{f} \\to u = \\frac{vf}{v - f}$$\r\n","1. Fix the focus distance $u$ (acquire scene points at a certain distance): $$\\frac{1}{u} + \\frac{1}{v} = \\frac{1}{f} \\to v = \\frac{uf}{u - f}$$\r\n","\r\n","Given the chosen position of the *image plane*, scene points in front and behind the respective *focus plane* will result out-of-focus, appearing in the images as circles (circle of confusion or blur circles).\r\n","\r\n","<center><img src=https://i.ibb.co/HqMB4gW/photo-2021-01-18-22-31-37.jpg width=\"600px\" /> </center>\r\n","\r\n","In practice, as long as blur circles are smaller than the size of the photosensing elements, the image will still look on-focus, this happens because the image plane is not continuous (pixels).\r\n","The range of distances across which the image appears on focus (due to blur circles being small enough) determines the depht of field (DOF).\r\n","\r\n","Cameras often deploy an adjustable diaphgram (iris) to control the amount of light gathered through the effective aperture of the lens.\r\n","The smaller the diaphgram aperture is, the larger is the DOF (smaller size of blur circles).\r\n","We can also enhance the power of the light source to capture very fast (but tiniy) objects.\r\n","\r\n","**F-number**: ratio of the focal lenght to the effective aperture of the lens ($\\frac{f}{d}$).\r\n","F-number discrete units are known as *stops* are usually reported on the diaphgram to allow the user to adjust the effective aperture.\r\n","\r\n","The higher is the chosen stop, the smaller is the diaphgram aperture (larger DOF). With smaller stops we have more light."]},{"cell_type":"markdown","metadata":{"id":"YDJFturWtHp2"},"source":["### Focusing mechanism\r\n","To focus on objects at diverse distances, a mechansim that allow the lens to translate along the optical axis (wrt a fixed position of the image plane) is provided. \r\n","\r\n","The mechanism has two end of movement. \r\n","At one end position ($v=f$) the camera is focused at infinity, then the mechanism allow the lens to be translated farther away from the image plane up to a certain maximum value (other end position), which determines the minimum focusing distance.\r\n","\r\n","<center><img src=https://i.ibb.co/LYDRsw6/photo-2021-01-19-09-27-52.jpg width=\"600px\" /> </center>"]},{"cell_type":"markdown","metadata":{"id":"erbTbt3lxSO8"},"source":["### Telecentric lenses\r\n","By placing a diaphgram with a small hole at the focal point of the lens it's possible to block all light rays but those parallel to the optical axis.\r\n","\r\n","<center><img src=https://i.ibb.co/ncrykMp/photo-2021-01-19-12-10-08.jpg width=\"600px\" /> </center>\r\n","\r\n","In this manner we realize an **ortographic projection**, that doesn't exhibit perspective projection, although, the size of the object can't be larger than the lens itself.\r\n"]},{"cell_type":"markdown","metadata":{"id":"g5sFVuF3WWJO"},"source":["### Fundamental radiometric relation\r\n","Let be:\r\n","* **Irradiance** $E$ (of a point on a surface): amount of light incident on that point;\r\n","* **Radiance** $L$ (of a surface point in any direction): amount of light emitted by the point in that direction.\r\n","The fundamental radiometric relation shows that the irradiance of an image point is proportional to the radiance of the corresponding scene point, along the viewing direction: $$E(p) = L(P) \\frac{\\pi}{4} {\\Bigl(\\frac{d}{f}\\Bigr)}^2 \\cos^4 \\alpha$$\r\n","\r\n","<center><img src=https://i.ibb.co/r5Jb8Rr/photo-2021-01-19-12-30-11.jpg width=\"400px\" /> </center>\r\n","\r\n","*Note*: the radiance depends on the power and position of the light sources, and on the reflectance properties of the material.\r\n"]},{"cell_type":"markdown","metadata":{"id":"ub96OTwJcZhw"},"source":["#### Reflectance of a surface\r\n","The latter property is usually described by a complex function called **Bi-Directional Reflectance Function** (BDRF), not always analytical and usually parametrized by two angles: azimut and elevation.\r\n","\r\n","That means that it's possible to determine the amount of light emitted in a certain direction given the amount of light received from the sources.\r\n","\r\n","There exists two simplified and opposite reflectance model:\r\n","* **Lambertian** (diffusely reflecting): the incoming light is spread equally across all viewing direction (appears equally bright from any view direction):\r\n","$$L(P) = \\rho(P) \\cdot E(P)$$\r\n","where $\\rho(P)$ is the albedo (the visible light);\r\n","* **Specular surface**: reflects the light coming from any direction in only one direction, so incident and reflected rays are coplanar and form the same angle wrt the normal at the surface point.\r\n","\r\n","Real surfaces show a mixed behaviour, so more sophisticated model are used.\r\n","\r\n","*Note*: correspondant point share similar neighbourhood of pixels, if the model is not Lambertian we have different light in different directions.\r\n"]},{"cell_type":"markdown","metadata":{"id":"hfhAJ0_ne4z9"},"source":["## Image digitization\r\n","The image plane of a camera consists of a planar sensor which convert the irradiance at any point into an electric quantity.\r\n","Afterwards, such continuous \"electric\" image is sampled and quantized to end up with a digital images (bits) suitable to visualization and processing by a computer.\r\n","\r\n","<center><img src=https://i.ibb.co/cbVRvv4/photo-2021-01-19-13-01-15.jpg width=\"700px\" /> </center>\r\n","\r\n","**Sampling**: the planar continuous image is sampled evenly along both the horizontal and vertical directions to pick up a 2d array (matrix) of $N \\times M$ samples known as pixels:\r\n","$$I(x,y) \\to \\begin{bmatrix} I(0,0) & \\dots & I(0, M - 1) \\\\ \\vdots & \\ddots & \\vdots \\\\ I(N - 1, 0) & \\dots & I(N - 1, M - 1) \\end{bmatrix} $$\r\n","**Quantization**: the continuous range of values associated with pixels is quantized into $l = 2^m$ discrete levels known as *grey-levels*.\r\n","\r\n","Letting $m$ be the number of bits used to represent a pixel (tipically a byte, $m = 8$), the memory occupancy in bits of a grey-scale image will be $B = N \\times M \\times m$.\r\n","\r\n","The more bits we spend for its representation, the higher the quality of the digital image (for both sampling and optimization)\r\n","\r\n","Lenna's story: http://www.lenna.org/."]},{"cell_type":"markdown","metadata":{"id":"ajZ2DV_Uzkkh"},"source":["### Digitization in practice\r\n","The sensor is a 2d array of photodetectors (i.e. photogates or photodiodes).\r\n","\r\n","During the exposure time, each detector converts the incident light into a proportionale electric charge (photons to electrons).\r\n","Then, the another circuit reads-out the charge, generating an output signal (can be digital or analogic, in case of digital an ADC converter is also needed).\r\n","\r\n","Hence, there is never a continuous image, since the image is sensed directly as a sample signal (row by row).\r\n","\r\n","*Note*: in analog cameras, the native sampling taking place at the sensor is lost in the generation of the analog output, that is then sampled and quantized by an **analog frame grabber**.\r\n","This means that pixels in a digital image coming from an analog camera do not correspond those sensed by the photodetectors.\r\n","In analog cameras the signal is continuous in rows but digital in columns."]},{"cell_type":"markdown","metadata":{"id":"9Bi9oEaK11cI"},"source":["## Camera parameters\r\n","**Signal-to-noise ratio** (SNR): the intensity (numeric value) measured at a pixel under perfectly static conditions varies due to the presence of random noise (the pixel value is not deterministic but more random variable)\r\n","\r\n","The SNR can be though as a quantifier for the strenght of the \"true\" signal wrt fluctuations due to noise.\r\n","It's usually expressed in decibels or bits (standard measures):\r\n","$$\\text{SNR}_{dB} = 20 \\log_{10}(\\text{SNR})$$\r\n","$$\\text{SNR}_{bit} = \\log_{2}(\\text{SNR})$$\r\n","\r\n","The main noise source are:\r\n","1. **Photon shot noise**: the number of photons collected during exposure time is not constant since it's a Poisson process;\r\n","1. **Electronic circuitry noise**: generated by the electronics that reads-out the charge;\r\n","1. **Quantization noise**: due to the final ADC conversion (in digital cameras);\r\n","1. **Dark current noise**: random amount of charge, due to thermal excitement observed at each pixel, even if the sensor is not exposed to light. \r\n","\r\n","<center><img src=https://i.ibb.co/LRrW2ZC/photo-2021-01-19-14-43-01.jpg width=\"700px\"/></center>\r\n","\r\n","De-noise an image is one of the main goal of pre-processing before using an algorithm.\r\n","\r\n","<center><img src=https://i.ibb.co/3zHS9Yr/photo-2021-01-19-14-46-30.jpg width=\"700px\"/></center>\r\n","\r\n","Since the process is not deterministic we have: $$I_i(p) = I_i^* + n(p)$$\r\n","where $I_i^*$ is a deterministc constant and $n(p)$ is the noise, a random variable.\r\n","Averaging on $n$ measurament: \r\n","$$\\widetilde{I}(p) = \\frac{1}{n} \\sum_{i = 1}^n I_i^*(p) + \\frac{1}{n} \\sum_{i = 1}^n n(p) = I^*(p)$$\r\n","due to the central limit theorem (noise has a normal distribution), random variables has null mean (for $n \\to \\infty$).\r\n","We need time to make it so (more images).\r\n","\r\n","Pixel's noise is independent, but if we don't have time for a temporal mean we can do it through space (in the neighbourhood).\r\n","\r\n","<center><img src=https://i.ibb.co/ZmQbCfj/photo-2021-01-19-15-17-43.jpg width=\"300px\"/></center>\r\n","\r\n","Hypothesizing the same amount of light while sliding the red window we have:\r\n","$$\\widetilde{I}(p) = \\frac{1}{s*} \\sum_{q \\in S} I(q)$$\r\n","\r\n","Note: usually denoising algorithm are embedded in camera's firmware.\r\n","\r\n","**Dynamic range**\r\n","\r\n","Let be:\r\n","* $E_\\min$: minimum detectable irradiation;\r\n","* $E_\\max$: the saturation irradiation (the maximum capacity of the photodetector).\r\n","\r\n","Then, the dynamic range will be $\\text{DR} = \\frac{E_\\max}{E_\\min}$.\r\n","\r\n","<center><img src=https://i.ibb.co/zJ5Yk1y/photo-2021-01-19-15-29-31.jpg width=\"800px\"/></center>\r\n","\r\n","The higher the DR, the better is the ability of the sensor to simultaneously capture in one image both dark and bright structures of the scene.\r\n","\r\n","**High dynamic range** (HDR): combining together a sequence of images of the same subject taken under different exposure times.\r\n","\r\n","**Sensitivity (or responsivity)**: deals with the amount of signal that the sensor can deliver per unit of input optical energy.\r\n","\r\n","**Uniformity (spatial or pattern noise)**: due to manifacturing tolerances both the response to light and the amount of dark noise vary across pixels."]},{"cell_type":"markdown","metadata":{"id":"XU53bYpSEtb4"},"source":["## CCD vs CMOD\r\n","CMOS technology allows the electronic circuitry to be integrated within the same chip as the sensor, providing compactness, less power consumption and lower costs.\r\n","\r\n","Moreover, CMOS sensors allow an arbitrary window to be read-out without having to receive the full image.\r\n","This can speeds up the inspection or tracking of a certain ROI (region of interest).\r\n","\r\n","On the other hand, CCD technology provides higher SNR, higher DR and better uniformity"]},{"cell_type":"markdown","metadata":{"id":"0lZr4vrMFyZD"},"source":["### Colour sensor\r\n","Sensors are sensitive to light ranging from near-ultraviolet, through the visible spectrum, up to the near infrared.\r\n","\r\n","The sensed intensity at a pixel results from an integration over the range of wavelenghts of the spectral distribution of the incoming light, multiplied by the spectral response function of the sensor.\r\n","\r\n","This means that CCD and CMOS sensors **cannot** sense colour.\r\n","<center><img src=https://i.ibb.co/DLgrjxr/photo-2021-01-19-15-41-53.jpg width=\"800px\"/></center>\r\n","\r\n","To create a colour sensor, an array of optical fibers (colour filter array) is placed in front of the photodetectors, to render each pixel sensitive to a specific range of wavelenghts.\r\n","\r\n","To obtain an RGB triplet at each pixel, missing pixels are interpolated from neighbouring pixels (**demosaicking**).\r\n","\r\n","\r\n","**Bayer CFA**: twice green filter like in human eye. In this manner the true resolution of the sensor is smalle due to the different sampling factor."]},{"cell_type":"markdown","metadata":{"id":"aRTtDBlxJe-g"},"source":["### Sensor sizes\r\n","CCD and CMOS sensors come in different sizes, which are specified in inches for the sake of legacy wrt old cameras based on cathode ray tubes. \r\n","The size of the diagonal of a solid state sensor is roughly $2/3$ of its size."]}]}